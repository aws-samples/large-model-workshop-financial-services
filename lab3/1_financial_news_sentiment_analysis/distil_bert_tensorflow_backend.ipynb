{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cab557d-37b3-4ccc-91ab-ecbfe055031f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "This notebook was tested with the TensorFlow 1.15 Python 3.7 (optimized for CPU) image and Python 3 kernel. Please make sure the correct kernel and image is et before executing this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e741a",
   "metadata": {},
   "source": [
    "# Notebook 1 - Financial News Sentiment Analysis\n",
    "\n",
    "## Use case\n",
    "Sentiment Analysis of Financial news provides practical guidance on the use of sentiment data in investment and trading strategies. It can assist financial trading and automatic trading systems to make investment decisions and enhance their predictive performance. Understanding and measuring the relationship of financial news with sentiment data can help analyst come up with financial forecasting. \n",
    "\n",
    "Financial news sentiment analysis is a type of natural language processing (NLP) that uses machine learning algorithms to analyze and classify the sentiment of financial news articles or other related content. The goal is to determine the overall sentiment of the news and identify whether the sentiment is positive, negative, or neutral.\n",
    "\n",
    "This analysis can help investors and traders make more informed decisions by identifying patterns in the sentiment of news articles that may influence financial markets. For example, if there is a lot of negative sentiment surrounding a particular company or industry, it could be an indication that the market will react negatively.\n",
    "\n",
    "Sentiment analysis in finance can also be used to track the performance of companies or financial products over time. By analyzing news articles and other sources of financial information, investors and traders can gain insight into how the market perceives certain companies or products.\n",
    "\n",
    "Overall, financial news sentiment analysis is a valuable tool for investors and traders who want to stay on top of market trends and make informed decisions based on the latest news and sentiment data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a6c38-4825-4618-a424-401076b23225",
   "metadata": {},
   "source": [
    "## What will you learn?\n",
    "\n",
    "In this notebook, we will walk you through how to use NVIDIA Triton Inference Server on Amazon SageMaker multi-model endpoints(MME) with GPU feature to deploy NLP models (**DistilBERT**) for financial news sentiment analysis.\n",
    "\n",
    "Amazon SageMaker MME provide a scalable and cost-effective way to deploy large number of deep learning models. Previously, customers had limited options to deploy 100s of deep learning models that need accelerated compute with GPUs. Now customers can deploy 1000s of deep learning models behind one SageMaker endpoint. Now, MME will run multiple models on a GPU, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve best price performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3362da4",
   "metadata": {},
   "source": [
    "## Installs\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server. Update SageMaker, boto3, awscli etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f7063",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex --quiet\n",
    "!pip install tritonclient[http] --quiet\n",
    "!pip install transformers[sentencepiece] --quiet\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da61f04",
   "metadata": {},
   "source": [
    "## Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70e0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"financial-usecase-mme\"\n",
    "\n",
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061deb53",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "This section presents overview of main steps for preparing DistilBERT TensorFlow model (served using TensorFlow backend) \n",
    "\n",
    "### 1. Generate Model Artifacts\n",
    "\n",
    "#### DistilBERT TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da72b4",
   "metadata": {},
   "source": [
    "First, we use HuggingFace transformers to load pre-trained DistilBERT TensorFlow model that has been fine-tuned for sentiment analysis binary classification task. Then, we save the model as SavedModel serialized format. It can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368026c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from transformers.utils import logging\n",
    "# set logging to ERROR and above\n",
    "logging.set_verbosity(40)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "print(\"Downloading pretrained DistilBERT Classification TF Model from HuggingFace...\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "save_model_dir = './workspace/hf_distilbert'\n",
    "print(\"Serializing Model to SavedModel file...\")\n",
    "model.save_pretrained(save_model_dir, saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30864171-7566-4d94-a33c-a31709a6cb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6179d95",
   "metadata": {},
   "source": [
    "### 2. Build Model Respository\n",
    "\n",
    "Using Triton on SageMaker requires us to first set up a [model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md) folder containing the models we want to serve. For each model we need to create a model directory consisting of the model artifact and define config.pbtxt file to specify [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load and serve the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285fe3e",
   "metadata": {},
   "source": [
    "#### DistilBERT TensorFlow Model\n",
    "\n",
    "Model repository structure for DistilBERT TensorFlow Model.\n",
    "\n",
    "```\n",
    "distilbert_tf\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.savedmodel\n",
    "â””â”€â”€ config.pbtxt\n",
    "```\n",
    "\n",
    "Model configuration must specify the platform and backend properties, max_batch_size property and the input and output tensors of the model. Additionally, you can specify instance_group and dynamic_batching properties for optimal inference performance in terms of latency and concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fd6e0",
   "metadata": {},
   "source": [
    "Below we set up the DistilBERT TensorFlow Model in the model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da5f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/distilbert_tf/1\n",
    "!cp -r workspace/hf_distilbert/saved_model/1 workspace/model.savedmodel\n",
    "!cp -r workspace/model.savedmodel model_repository/distilbert_tf/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d88880",
   "metadata": {},
   "source": [
    "Then we define its config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20b9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model_repository/distilbert_tf/config.pbtxt\n",
    "name: \"distilbert_tf\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size: 8\n",
    "input: [\n",
    "    {\n",
    "        name: \"input_ids\"\n",
    "        data_type: TYPE_INT32\n",
    "        dims: [ -1 ]\n",
    "    },\n",
    "    {\n",
    "        name: \"attention_mask\"\n",
    "        data_type: TYPE_INT32\n",
    "        dims: [ -1 ]\n",
    "    }\n",
    "]\n",
    "output: [\n",
    "    {\n",
    "        name: \"logits\"\n",
    "        data_type: TYPE_FP32\n",
    "        dims: [ 2 ]\n",
    "    }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396b213",
   "metadata": {},
   "source": [
    "### 3. Package models and upload to S3\n",
    "\n",
    "Next, we will package our models as `*.tar.gz` files for uploading to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad54ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -C model_repository/ -czf distilbert_tf.tar.gz distilbert_tf\n",
    "model_uri_distilbert_tf = sagemaker_session.upload_data(path=\"distilbert_tf.tar.gz\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b46275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uri_distilbert_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932a793",
   "metadata": {},
   "source": [
    "### 4. Create SageMaker Endpoint\n",
    "\n",
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker multi-model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121296d",
   "metadata": {},
   "source": [
    "#### Define the serving container\n",
    "In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load and serve predictions. Set `Mode` to `MultiModel` to indicate SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f75fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fd2a7",
   "metadata": {},
   "source": [
    "#### Create a multi-model object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284650f",
   "metadata": {},
   "source": [
    "Once the image, data location are set we create the model using `create_model` by specifying the `ModelName` and the Container definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257d13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61995a",
   "metadata": {},
   "source": [
    "#### Define configuration for the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7aa472",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint. Here we are deploying to `g5.2xlarge` NVIDIA GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e8fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6e34e",
   "metadata": {},
   "source": [
    "#### Create Multi-Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b515e9d",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74096ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2513f9b-fca9-4b9e-ad48-9220b38aa728",
   "metadata": {},
   "source": [
    "## NOTE: we will use this endpoint_name in next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aedaeb-a026-400a-8f56-4e5ce97017f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Endpoint Name: \" + endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ec901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4771cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store sm_model_name\n",
    "%store endpoint_config_name\n",
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a176f",
   "metadata": {},
   "source": [
    "### 5. Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7354910",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using JSON as the payload format. For the inference request format, Triton uses the KFServing community standard [inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b8e6e",
   "metadata": {},
   "source": [
    "#### Add utility methods for preparing JSON request payload\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25447735",
   "metadata": {},
   "source": [
    "We'll use the following utility methods to convert our inference request for DistilBERT and T5 models into a json payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe5fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_text(model_name, text):\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    tokenized_text = tokenizer(text, padding=True, return_tensors=\"np\")\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask\n",
    "\n",
    "def get_text_payload(model_name, text):\n",
    "    input_ids, attention_mask = tokenize_text(model_name, text)\n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = []\n",
    "    payload[\"inputs\"].append({\"name\": \"input_ids\", \"shape\": input_ids.shape, \"datatype\": \"INT32\", \"data\": input_ids.tolist()})\n",
    "    payload[\"inputs\"].append({\"name\": \"attention_mask\", \"shape\": attention_mask.shape, \"datatype\": \"INT32\", \"data\": attention_mask.tolist()})\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bba032",
   "metadata": {},
   "source": [
    "#### Invoke target model on Multi Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169959e",
   "metadata": {},
   "source": [
    "We can send inference request to multi-model endpoint using `invoke_enpoint` API. We specify the `TargetModel` in the invocation call and pass in the payload for each model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5693af",
   "metadata": {},
   "source": [
    "#### DistilBERT TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3d4d6",
   "metadata": {},
   "source": [
    "##### Sample DistilBERT Inference using Json Payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8641144",
   "metadata": {},
   "source": [
    "First, we show some sample inference on the DistilBERT TensorFlow Binary Classification Model deployed on Triton's TensorFlow SavedModel Backend behind SageMaker MME GPU endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61acfbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts_to_classify = [\"One of the challenges in the oil production in the North Sea is scale formation that can plug pipelines and halt production .\",\n",
    "                     \"With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .\"]\n",
    "batch_size = len(texts_to_classify)\n",
    "\n",
    "distilbert_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "distilbert_payload = get_text_payload(distilbert_model, texts_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b48cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(distilbert_payload),\n",
    "    TargetModel=\"distilbert_tf.tar.gz\",\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "logits = np.array(response_body[\"outputs\"][0][\"data\"]).reshape(batch_size, -1)\n",
    "CLASSES = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predictions = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    pred_class_idx = np.argmax(logits[i])\n",
    "    predictions.append(CLASSES[pred_class_idx])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5509e",
   "metadata": {},
   "source": [
    "##### Sample DistilBERT Inference using Binary + Json Payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a417e5",
   "metadata": {},
   "source": [
    "We can also use `binary+json` as the payload format to get better performance for the inference call. The specification of this format is provided [here](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md).\n",
    "\n",
    "**Note:** With the binary+json format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `Application/vnd.sagemaker-triton.binary+json;json-header-size={}`.\n",
    "\n",
    "Please note, this is different from using `Inference-Header-Content-Length` header on a stand-alone Triton server since custom headers are not allowed in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f5b39",
   "metadata": {},
   "source": [
    "The `tritonclient` package in Triton provides utility methods to generate the payload without having to know the details of the specification. We'll use the following method to convert our inference request for DistilBERT and T5 models into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521dae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "def get_text_payload_binary(model_name, text):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_ids, attention_mask = tokenize_text(model_name, text)\n",
    "    inputs.append(httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT32\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int32), binary_data=True)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int32), binary_data=True)\n",
    "    \n",
    "    output_name = \"output\" if model_name == \"t5-small\" else \"logits\"\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143d1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body, header_length = get_text_payload_binary(distilbert_model, texts_to_classify)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='distilbert_tf.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "output_name = \"logits\"\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "logits = result.as_numpy(output_name)\n",
    "CLASSES = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predictions = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    pred_class_idx = np.argmax(logits[i])\n",
    "    predictions.append(CLASSES[pred_class_idx])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e67bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14c93b-0bf3-403f-afa8-3884a9ca1a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155661e-349c-45e0-97c6-7a324ae5bedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 1.15 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-1.15-cpu-py37-ubuntu18.04-v7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
